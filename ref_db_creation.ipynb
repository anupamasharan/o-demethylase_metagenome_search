{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generating BLAST initial output with desired template sequences\n",
    "The sequences chosen for this step has been though literature search, utilising experimentally produced and tested candidates from two different model organisms for this enzyme sysetm, *Acetobacerium dehalogenans* [[1]](https://doi.org/10.1128/JB.01104-08) and *Desulfitobacterium hafinase* [[2]](https://doi.org/10.1111/1574-6941.12433) as template sequences to produce the database. We will be searching for the protein products separately. To keep the markdown short, examples will be given with one protein group at a time (either CP, MT1 or MT2) and the same steps will usually be repeated for the other groups.\n",
    "\n",
    "Install Biopython if you have not done it before by removing the comment from the first line. To understand how to manipulate the parameters of functions within the common biopython modules such as blast, refer to the biopython cookbook [[3]](http://biopython.org/DIST/docs/tutorial/Tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install biopython\n",
    "import Bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading modules needed to run and parse BLAST output from within Biopython\n",
    "from Bio.Blast import NCBIWWW\n",
    "from Bio.Blast import NCBIXML\n",
    "\n",
    "#defining a function for generating BLAST output from a given fasta file (containing single or multiple accession IDs)\n",
    "\n",
    "def generate_blast_record():\n",
    "    fasta_string = input('Enter_path_to_file: ')\n",
    "    fasta_string_in = open(fasta_string)\n",
    "    result_handle = NCBIWWW.qblast(\"blastp\", \"refseq_protein\", fasta_string_in.read(), url_base='https://blast.ncbi.nlm.nih.gov/Blast.cgi', expect= 1e-10, hitlist_size = 100)\n",
    "    #please update the parameters as per your requirements\n",
    "    with open(\"my_blast_run.xml\", \"w\") as out_handle:\n",
    "        out_handle.write(result_handle.read())\n",
    "    result_handle.close()\n",
    "    result_handle = open(\"my_blast_run.xml\")\n",
    "    blast_records = NCBIXML.parse(result_handle)\n",
    "    return(blast_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input the file and save the handle for vieiwing later\n",
    "blast_record_list_CP = []\n",
    "blast_record_list_CP = list(generate_blast_record())\n",
    "print(blast_record_list_CP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CP blast record dataframe creation by parsing through the output\n",
    "\n",
    "all_ID_list_CP_demo = []\n",
    "all_hit_length_list_CP_demo = []\n",
    "all_hit_evalue_list_CP_demo = []\n",
    "for blast_record in blast_record_list_CP:\n",
    "    for alignment in blast_record.alignments:\n",
    "        all_ID_list_CP_demo.append(alignment.title)\n",
    "        all_hit_length_list_CP_demo.append(alignment.length)\n",
    "        for hsp in alignment.hsps:\n",
    "            all_hit_evalue_list_CP_demo.append(hsp.expect)\n",
    "print(all_ID_list_CP_demo, all_hit_length_list_CP_demo, all_hit_evalue_list_CP_demo) #checkpoint for loop output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'> Note: </font> For determining co-localised indices, only a few parameters of the blast output have been imported from the record handle. For a more thorough investigation, the other parameters such as percentage identity, can also be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zipping lists together to make the dataframe\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_demo_CP = pd.DataFrame(list(zip(all_ID_list_CP_demo, all_hit_length_list_CP_demo, all_hit_evalue_list_CP_demo)), columns = ['CP_description', 'CP_length', 'CP_e-value'])\n",
    "#some duplicates will also be returned as we have three template sequences that might match differently with the three data\n",
    "df_demo_CP_no_dups = df_demo_CP.drop_duplicates(subset=['CP_description'])\n",
    "print(df_demo_CP_no_dups.head())\n",
    "\n",
    "#sanity check by printing length of both dfs\n",
    "print(len(df_demo_CP))\n",
    "print(len(df_demo_CP_no_dups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'> Note: </font> Always check for duplicates, especially with multiple queries in blast input as shown here. It is also recommended to create checkpoints at each step so that you have the most latest dataframe in case you need to start over at some point later. This is optional though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo_CP.to_csv(\"ref_db_creation/data/CP_step1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Tidying BLAST output and prep for analysis\n",
    "<br>\n",
    "<font color = 'red'> Note: </font> The data wrangling demonstrated here is specific to the output generated in Step 1 using the given files. However, it is easily adaptable to any similar dataset or trying to tidy and parse through blast output generated by biopython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to split the description column so that it is easy to manipulate for evidence of co-localisation\n",
    "\n",
    "# 1 : splitting based on '|' character to get 3 columns\n",
    "\n",
    "df_tidy_CP = pd.DataFrame()\n",
    "df_tidy_CP[['CP_blastdb_ID', 'CP_ID', 'CP_annotation_full']] = df_master_CP.description_CP.str.split(\"|\", expand = True,)\n",
    "df_tidy_CP[['CP_length', 'CP_evalue']] = df_master_CP[['length_CP', 'e-value_CP']]\n",
    "print(df_tidy_CP.head())\n",
    "print(len(df_tidy_CP)) #keep checking length in tidying steps\n",
    "df_tidy_CP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 : Splitting annotation column further\n",
    "\n",
    "df_tidy_CP[['1', '2']] = df_tidy_CP.CP_annotation_full.str.split('[', expand = True,)\n",
    "print(df_tidy_CP.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'> Note: </font> It is important to note that the number of columns given for expansion of column based on character separator can be arbitrary, all NCBI records might not have the same format for organism name and therefore this should be carefully evaluated. Usually python will prompt this and return an \"unequal\" column length error similar to the screenshot below:\n",
    "<br>\n",
    "<img src = \"images/tidy_step_error.png\" width=\"900\" height=\"1000\" align=\"centre\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 : Stitching organism name together and final tidying\n",
    "\n",
    "df_tidy_CP['2'] = df_tidy_CP['2'].str.replace(']','') #cleaning the column\n",
    "df_tidy_CP = df_tidy_CP.drop('CP_annotation_full', axis = 1) #dropping unwanted columns\n",
    "df_tidy_CP = df_tidy_CP[['CP_ID', '1', '2', 'CP_length', 'CP_evalue']] #reordering for one last time\n",
    "desired_column_names_CP = ['CP_ID', 'CP_annotation', 'CP_source_organism', 'CP_length', 'CP_e-value'] #assigning tidy names\n",
    "df_tidy_CP.columns = desired_column_names_CP\n",
    "print(df_tidy_CP.isnull().sum()) #checking that there are no null values\n",
    "df_tidy_CP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tidy_CP_final = df_tidy_CP # assigning a new checkpoint df at end of step 2, good to keep exporting files at different steps\n",
    "df_tidy_CP_final.to_csv(\"data/Step_2_CP_demo.csv\", index = False) #saving the csv file\n",
    "df_tidy_CP_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to extract headers into a separate csv file for the next step\n",
    "header = [\"CP_ID\"]\n",
    "df_tidy_CP_final.to_csv('all_headers_CP.csv', columns = header, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same steps were repeated for the MT1 and MT2 blast output to generate the final list of headers. This header file is the input for the python script to mine the co-localised headers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
